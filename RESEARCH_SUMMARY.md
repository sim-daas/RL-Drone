# RL Drone Navigation Research Summary

## üéØ Final Achievement

**100% Success Rate** in obstacle avoidance navigation using Soft Actor-Critic (SAC)

| Metric | Result |
|--------|--------|
| **Goal Success** | 100% (5/5) |
| **Collision Rate** | 0% |
| **Average Reward** | +587 |
| **Episode Length** | 463 steps (~15s) |
| **Training Time** | 1M steps |

---

## Experimental Journey

### Iteration 1: Complex Dynamic Rewards ‚ùå
- **Issue:** Gradient explosion from massive rewards (¬±3000)
- **Result:** 0% success, erratic bouncing
- **Lesson:** Keep rewards bounded

### Iteration 2: Simplified Rewards ‚ùå  
- **Issue:** Missing yaw control caused rotation
- **Result:** Improved stability, still 0% success
- **Lesson:** Control mode matters

### Iteration 3: Better Goal Representation ‚ö†Ô∏è
- **Change:** Distance + direction vs raw offset
- **Result:** Clearer learning signal
- **Lesson:** Input representation is critical

### Iteration 4 (Phase 1): Full Mode 6 Control ‚ö†Ô∏è
- **Changes:** 4D actions [vx, vy, vr, vz], Z-ceiling (2.5m)
- **Result:** Extremely stable, but just hovers (0% success)
- **Lesson:** Stability achieved, but no goal-seeking

### Iteration 5 (Phase 2): Progress Rewards ‚úÖ **SUCCESS!**
- **Changes:** +20 √ó delta for progress, -0.1 for hovering
- **Result:** **100% success!** Single goal navigation mastered
- **Lesson:** Progress incentives are essential

### Iteration 6 (Phase 3): Multi-Goal + Distance-Dependent Rewards ‚ö†Ô∏è **MIXED**

**Changes:**
- **10 goal positions:** x=18, y‚àà[-5, 5] (even spacing), z=1
- **Random goal per episode** for generalization
- **Distance-dependent rewards:**
  - Near goal (<1.5m): 30√ó reward, 0.006m threshold, -0.01 penalty
  - Far from goal (‚â•1.5m): 20√ó reward, 0.001m threshold, -0.1 penalty

**Results (6M total steps):**
- ‚úÖ **Excellent generalization** across different y-positions
- ‚úÖ **Fast navigation** - drone reaches goals efficiently
- ‚ö†Ô∏è **Worse near-goal precision** compared to Phase 2
- ‚ö†Ô∏è **Trade-off discovered:** Complex rewards help one aspect, hurt another

**Analysis:**

**What Worked:**
- Multi-goal training successful - generalizes well
- Fast reaching behavior maintained
- No catastrophic forgetting from Phase 2

**What Regressed:**
- Near-goal precision decreased
- Distance-dependent rewards added complexity
- Fine-tuning near goal became harder (smaller penalty = less deterrent)

**Root Cause:**
```
Phase 2 (simple, uniform):
- Consistent incentive structure everywhere
- Agent learned precise positioning

Phase 3 (complex, distance-dependent):
- Different rules near vs far from goal
- Agent confused about final positioning
- Smaller penalty (-0.01) insufficient deterrent near goal
```

**Lessons Learned:**
1. ‚ùå **Complexity doesn't always help** - simpler may be better
2. ‚ö†Ô∏è **Trade-offs exist** - optimizing for one metric can hurt another
3. ‚úÖ **Multi-goal training works** - good foundation for generalization
4. üîÑ **Consider reverting** to uniform rewards (Phase 2 style)
5. üí° **Alternative:** Reduce both positive and negative rewards near goal (symmetrical)

### Iteration 7 (Phase 4): Diverse 2D Goals ‚ùå **REGRESSION**

**Changes:**
- **Reverted to uniform Phase 2 rewards** (20√ó delta, 0.006m threshold, -0.1 penalty)
- **10 diverse goals** across 36m √ó 36m area (not single line)
- **Per-episode goal randomization** in GymEnv.reset()
- **Dynamic max_steps:** `distance √ó 1.2 √ó 30Hz`
- **Dynamic terminal rewards:** ¬±proportional to initial_distance
- **6 parallel envs, 1 thread each**

**Results (2-3M steps from Phase 3 checkpoint):**

| Metric | Result |
|--------|--------|
| **Navigation** | ‚ùå Random smooth movement, no goal alignment |
| **Collision avoidance** | ‚úÖ Preserved (smooth, stable flight) |
| **Altitude hold** | ‚úÖ Preserved |
| **Mean reward** | 0 to +100 (never reaching goals) |
| **Episode length** | ~500 steps (constant, never shortening) |
| **Goal independence** | Same random behavior for opposite-direction goals |

**TensorBoard Analysis:**
- Mean episode length: ~500 steps, no improvement over training
- Mean reward: Increased slightly in first 1.5M steps to positive
- After 1.5M: Plateaued between 0-100, slight increase after 2.5M (max ~150)
- No goal-reaching events observed

**Root Cause: Too Much Diversity, Too Fast**
```
Phase 2 model: Learned ONE path (start ‚Üí [18, -5, 1])
Phase 4 goals: 10 positions spanning 36m √ó 36m, including OPPOSITE directions

Agent's response:
- "I know how to hover safely" ‚úÖ (preserved)
- "I don't know where ANY of these goals are" ‚ùå (overwhelmed)
- Falls back to safe hovering behavior
```

**Key distances showing the problem:**

| Goal | Direction | Distance |
|------|-----------|----------|
| `[8, 5]` | Forward-right | ~11m |
| `[-17, -10]` | Behind-left | ~20m |
| `[10, -22]` | Right-far behind | ~22m |
| `[18, 14]` | Forward-far right | ~28m |

**Lessons Learned:**
1. ‚ùå **Diverse goals can't be introduced all at once** from a single-goal policy
2. ‚ùå **More training won't fix this** ‚Äî task distribution is the bottleneck, not training time
3. ‚úÖ **Stability/collision avoidance is robust** ‚Äî survives major goal changes
4. üí° **Curriculum learning is essential** ‚Äî gradual difficulty increase needed
5. üí° **Start with close, multi-directional goals** before extending range

---

## Curriculum Learning Plan (Phase 4A ‚Üí 4B ‚Üí 4C)

### Phase 4A: Close Goals (7-12m, 360¬∞ coverage)
```python
# 8 goals in a ring around start position
PHASE_A_GOALS = [
    [8.0, 0.0, 1.0],     # ~10m forward
    [5.0, 5.0, 1.0],     # ~9m forward-right
    [0.0, 8.0, 1.0],     # ~10m right
    [-5.0, 5.0, 1.0],    # ~9m back-right
    [-8.0, 0.0, 1.0],    # ~10m behind
    [-5.0, -5.0, 1.0],   # ~8m back-left
    [5.0, -5.0, 1.0],    # ~8m forward-left
    [0.0, -8.0, 1.0],    # ~10m left
]
```
**Goal:** Learn goal-directed flight in ANY direction  
**Train:** 2-3M steps | **Target:** >80% success

### Phase 4B: Medium Goals (12-18m)
Mix of close + medium goals  
**Train:** 2-3M steps | **Target:** >70% success

### Phase 4C: Full Range (18-28m)
All original diverse goals  
**Train:** 2-3M steps | **Target:** >60% success

## Key Technical Decisions

### Control System
```python
# Action Space: Full Mode 6 (4D)
[vx, vy, vr, vz]  # Ground velocities + yaw rate + vertical
Limits: [¬±5.0, ¬±5.0, ¬±2.0, ¬±5.0]
```

### Observation Space (50 dims)
- Attitude: 13 (velocities, position, orientation)
- Previous action: 4
- Auxiliary: 4
- **Lidar: 24 rays** (15¬∞ spacing - reduced from 60)
- **Goal distance: 1** (explicit scalar)
- **Goal direction: 3** (normalized unit vector)
- **Current yaw: 1** (heading awareness)

### Reward Function (Phase 2 - Successful)
```python
delta = previous_distance - current_distance

if delta > 0.006:  # Moving toward goal
    reward = +20.0 * delta
elif delta < -0.006:  # Moving away
    reward = -0.1
else:  # Hovering
    reward = -0.1

# Terminal rewards
goal_reached: +500
collision/OOB: -100
```

---

## Critical Insights

### 1. **Progress Rewards Are Essential** ‚≠ê‚≠ê‚≠ê
Pure penalty-based rewards led to risk-averse hovering rather than goal-seeking.

**Before (Phase 1):**
- Reward for hovering: -90 (safest)
- Reward for moving: -70 to -170 (risky)
- **Result:** Agent hovers

**After (Phase 2):**
- Reward for approaching goal: +10 per good step
- Reward for hovering: -0.1
- **Result:** 100% success

### 2. **Control Authority Matters**
- 3D actions [vx, vy, vz]: Drone rotates unpredictably ‚ùå
- 4D actions [vx, vy, vr, vz]: Stable heading control ‚úÖ

### 3. **Reward Scale Matters**
- 20√ó multiplier on distance provides strong learning gradient
- Threshold (0.006m) filters sensor noise effectively

### 4. **Simplicity Works**
- 24 lidar rays sufficient (vs 60 rays)
- 40% faster training, no performance loss

### 5. **Two-Phase Approach**
- Phase 1: Learn stability and control
- Phase 2: Add goal-seeking behavior
- **Combined = Success**

---

## System Configuration

```python
# SAC Hyperparameters
LEARNING_RATE = 3e-4
BUFFER_SIZE = 300,000
BATCH_SIZE = 256
PARALLEL_ENVS = 6
TOTAL_STEPS = 1,000,000

# Environment
EPISODE_LENGTH = 30s (900 steps)
MAX_ALTITUDE = 2.5m
FLIGHT_DOME = 100m
AGENT_HZ = 30Hz
```

---

## Research Contributions

1. **Validated:** Progress-based rewards critical for navigation tasks
2. **Validated:** Full Mode 6 control essential for stable learning
3. **Validated:** 24 lidar rays sufficient for warehouse navigation
4. **Discovered:** 20√ó reward scaling creates effective gradient
5. **Discovered:** 0.006m threshold optimal for filtering noise

---

## Training Timeline

- **Phase 1**: 1M steps ‚Üí Stability (0% success ‚Üí stable hovering)
- **Phase 2**: 1M steps ‚Üí Single-goal mastery (100% success)
- **Phase 3**: 4M steps ‚Üí Multi-goal generalization (good coverage, reduced precision)
- **Total**: 6M steps from scratch to multi-goal navigation

---

## Future Directions

### **Phase 4 Recommendations** (Based on Phase 3 Observations)

**Problem:** Distance-dependent rewards caused worse near-goal precision despite better generalization

#### **Option A - Revert to Uniform Rewards** ‚≠ê **RECOMMENDED**
```python
# Simple, consistent everywhere (Phase 2 style)
delta = previous_distance - current_distance
THRESHOLD = 0.006  # Same threshold everywhere

if delta > THRESHOLD:
    reward = 20.0 * delta  # Uniform scaling
elif delta < -THRESHOLD:
    reward = -0.1  # Uniform penalty
else:
    reward = -0.1  # Hovering penalty
```
**Pros:** ‚úÖ Simpler, ‚úÖ Proven in Phase 2, ‚úÖ Better precision  
**Cons:** ‚ö†Ô∏è May need more exploration far from goal

#### **Option B - Symmetrical Reduction Near Goal**
```python
if current_distance < 1.5:
    # Reduce BOTH rewards and penalties proportionally
    progress_reward = 10.0 * delta  # Half of normal
    retreat_penalty = -0.05  # Half of normal
else:
    progress_reward = 20.0 * delta
    retreat_penalty = -0.1
```
**Pros:** ‚úÖ Maintains distance awareness, ‚úÖ More balanced  
**Cons:** ‚ö†Ô∏è Still complex, ‚ö†Ô∏è Unproven

#### **Option C - Dynamic Episode Duration** üöÄ **USER SUGGESTED**
```python
# Adaptive timeout based on initial distance
self.max_duration = self.initial_distance / 2  # seconds
# Example: 18m distance ‚Üí 9 second timeout
#          25m distance ‚Üí 12.5 second timeout
```
**Pros:** ‚úÖ Adaptive, ‚úÖ Efficient, ‚úÖ Scales with difficulty  
**Cons:** ‚ö†Ô∏è May be too aggressive for obstacle-heavy paths  
**Mitigation:** Use factor between 0.5-0.75 for more tolerance

#### **Option D - Diverse 2D Goal Distribution** üéØ **USER SUGGESTED**
```python
# Not just single line (y-axis), but 2D distribution
GOAL_POSITIONS = [
    [x, y, 1.0] 
    for x in [15, 18, 21]  # 3 x-values
    for y in [-5, -2, 0, 2, 5]  # 5 y-values
]  # Total: 15 goals across x-y plane
```
**Pros:** ‚úÖ Better generalization, ‚úÖ More robust, ‚úÖ Tests diverse paths  
**Cons:** ‚ö†Ô∏è Longer training time needed

---

### **Recommended Phase 4 Configuration**

**Combine best practices from all phases:**

1. **Rewards:** Revert to uniform Phase 2 system (Option A)
2. **Goals:** 2D distribution (Option D) - 15-20 diverse positions
3. **Duration:** Dynamic timeout (Option C) with `max_duration = initial_distance / 1.5`
4. **Training:** 2-3M steps with simplified setup

**Expected Code Changes:**
```python
# train_sac.py / gymenv.py
GOAL_POSITIONS = [
    [x, y, 1.0] 
    for x in [15.0, 18.0, 21.0]
    for y in np.linspace(-5, 5, 6)  # 6 y-values per x
]  # 18 total goals

# gymenv.py - __init__
self.initial_distance = np.linalg.norm(self.goal_position - start_pos)
self.max_duration = self.initial_distance / 1.5  # Adaptive
self.max_steps = int(self.max_duration * self.agent_hz)

# gymenv.py - reward function (revert to Phase 2)
delta = self.previous_distance - current_distance
THRESHOLD = 0.006

if delta > THRESHOLD:
    self.reward = 20.0 * delta
elif delta < -THRESHOLD:
    self.reward = -0.1
else:
    self.reward = -0.1
```

**Expected Outcomes:**
- ‚úÖ Regain Phase 2 precision
- ‚úÖ Maintain Phase 3 generalization
- ‚úÖ Faster training with adaptive timeouts
- ‚úÖ Robust policy across diverse x-y positions

---

## Conclusion

**Journey Summary:**
- **Iterations 1-3:** Learning control fundamentals (2M+ failed attempts)
- **Phase 1 (Iteration 4):** Achieved stability (1M steps)
- **Phase 2 (Iteration 5):** **Breakthrough** - mastered single-goal (1M steps, 100% success)
- **Phase 3 (Iteration 6):** Multi-goal generalization (4M steps, trade-offs discovered)

**Key Discovery:** **Simpler is often better** - Phase 2's uniform rewards outperformed Phase 3's complex distance-dependent system for precision, while multi-goal training successfully improved generalization.

**Current Status:** 
- ‚úÖ Good generalization across y-positions
- ‚úÖ Fast navigation speed
- ‚ö†Ô∏è Needs precision improvement near goal

**Next Phase:** Combine Phase 2 simplicity + Phase 3 diversity + new features (dynamic duration, 2D goals) for robust, general-purpose navigation

---

*Framework: PyFlyt + SAC + Stable Baselines3*  
*Training: 6M steps total across 3 phases*  
*Hardware: AMD EPYC CPU, 6 parallel environments*  
*Date: January 2026*

